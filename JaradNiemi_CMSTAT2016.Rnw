\documentclass[handout,10pt]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\graphicspath{{include/}}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

% to avoid counting all pages
\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}


\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\newcommand{\ind}{\stackrel{ind}{\sim}}
\providecommand{\ov}[1]{\overline{#1}}


\institute[ISU]{Iowa State University}
\date{\today}

\title[pMCMC on GPUs for RNAseq]{Parallelized Markov chain Monte Carlo algorithms utilizing GPUs with an application to RNAseq data analysis}
\author{Jarad Niemi and Will Landau}

\begin{document}



\begin{frame}
\maketitle

\vspace{0.2in}

{\small
This research was supported by National Institute of General Medical Sciences (NIGMS) of the National Institutes of Health and the joint National Science Foundation / NIGMS Mathematical Biology Program under award number R01GM109458. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the National Science Foundation.
}

\end{frame}



\begin{frame}
\frametitle{Outline}

\begin{itemize}
\item Background
	\begin{itemize}
	\item Heterosis
	\item RNAseq data \pause
	\end{itemize}
\item Modeling 
	\begin{itemize}
	\item Hierarchical overdispersed count regression model 
	\end{itemize}
\item Fully Bayes \pause on graphics processing units
	\begin{itemize}
	\item Minimizing memory transfers between GPU and CPU \pause
	\end{itemize}
\item Simulation studies
	\begin{itemize}
	\item Credible interval coverage
	\item Heterosis detection via ROC curves \pause
	\end{itemize}
\item Real data analysis
\end{itemize}

\end{frame}


\section{Background}
\subsection{Heterosis}

\begin{frame}
\frametitle{Heterosis}
\begin{definition}
Heterosis, or hybrid vigor, is the enhancement of the phenotype of hybrid progeny relative to their inbred parents.
\end{definition}

\pause

\begin{center}
\includegraphics{heterosis}
\end{center}
{\tiny (\url{http://www2.iastate.edu/~nscentral/news/06/may/vigor.shtml} modified by Will Landau)} 
\end{frame}



\subsection{RNAseq data}
% \begin{frame}
% \frametitle{RNAseq data}
% \setkeys{Gin}{width=0.8\textwidth}
% \begin{center}
% \includegraphics{rnaseq}
% \end{center}
% {\tiny Wang, Gerstein, and Snyder. (2010) \url{http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2949280/figure/F1/}}
% \end{frame}


% \begin{frame}
% \frametitle{RNAseq data}
% \begin{center}
% \includegraphics{rnaseq2}
% \end{center}
% {\tiny url{http://bio.lundberg.gu.se/courses/vt13/rnaseq.html}}
% \end{frame}


\begin{frame}
\frametitle{RNAseq data}
\setkeys{Gin}{width=0.7\textwidth}

\begin{center}
\includegraphics{data}
{\tiny (Will Landau)}
\end{center}

\pause
{\small
\begin{itemize}
\item Low parent heterosis (LPH): expression in hybrid is lower than both parents
\item High parent heterosis (HPH): expression in hybrid is higher than both parents
\end{itemize}
}
\end{frame}


\section{Modeling}
\begin{frame}
\frametitle{Posterior hypothesis probabilities}
\small

If we had a posterior distribution for the mean expression levels, i.e. 
\[ 
p(\mu_B,\mu_M,\mu_{BM},\mu_{MB}|y)
\]
where $y$ represents our data, 
then we could calculate the relevant hypothesis probabilities, i.e. 
\[ \begin{array}{rl}
P(H_{0\phantom{PH}}|y) &= P(\mu_{min} < \mu_{BM} < \mu_{max}|y) \\
P(H_{LPH}|y) &= P(\mu_{BM} < \mu_{min}|y) \\
P(H_{HPH}|y) &= P(\mu_{max} < \mu_{BM}|y)
\end{array} \] 
where $\mu_{min} = \min(\mu_B,\mu_M)$ and $\mu_{max} = \max(\mu_B,\mu_M)$.

\vspace{0.1in} \pause

Hierarchical modeling \emph{partially pools} the information across genes and thereby provides a data-based multiple comparison adjustment by 
\begin{itemize}
\item shrinking estimates toward a grand mean (or zero) based on the variability inherent in the data and 
\item reducing posterior uncertainty by borrowing information across genes. 
\end{itemize}
{\tiny (Gelman, Hill, and Yajima (2012))}

\end{frame}


\subsection{Hierarchical overdispersed count regression model}
\begin{frame}
\frametitle{Overdispersed count regression model}

Let 
\begin{itemize}
\item $g$ ($g=1,\ldots,G$) identify the gene, 
\item $n$ ($n=1,\ldots,N$) identify the sample, \pause
\item $y$ be the $G\times N$ matrix of RNAseq counts \pause and
\item $X$ be the $N\times L$ model matrix that connects the $N$ samples to the varieties, blocking factors, etc.
\end{itemize}
\pause
We assume 
\[ 
y_{gn} \ind \text{Po} \left (e^{h_n + \varepsilon_{gn} + x_n' \beta_{g}} \right )
\]
\pause
where 
\begin{itemize}[<+->]
\item $h_n$ are \emph{normalization factors},
\item $\varepsilon_{gn} \ind N(0,\gamma_g)$ allow for gene-specific overdispersion, 
\item $x_n$ is the $n^{th}$ row of $X$, and
\item $\beta_g$ is a vector of length $L$ that account for effects on gene expression of variables of interest. 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Hierarchical model}

Recall 
\[ 
y_{gn} \ind \text{Po} \left (e^{h_n + \varepsilon_{gn} + x_n' \beta_{g}} \right )
\qquad\mbox{and}\qquad
\varepsilon_{gn} \ind N(0,\gamma_g).
\]

\vspace{0.1in} \pause

We construct a hierarchical model for both $\gamma_g$ and $\beta_g$ to borrow information across genes. \pause
Specifically, we assume
\[ 
1/\gamma_g\ind \text{Ga} \left (\nu/2,\nu\tau/2 \right)
\]
such that $E[1/\gamma_g] = 1/\tau$ and $CoV[1/\gamma_g] = \sqrt{2/\nu}$ \pause and 
\[ 
\beta_{g\ell} \ind N(\theta_\ell, \sigma_\ell^2)
\]
for $\ell=1,\ldots,L$. 
\end{frame}



\begin{frame}
\frametitle{Model matrix for our heterosis experiment}

Experimental design:  4 varieties, 2 plates, 2 replicates per variety per plate

\pause

\begin{equation*} 
X = \left (
\begin{bmatrix}
1 & \phantom{-}1 & -1 & \phantom{-}0 \\
1 & -1 & \phantom{-}1 & \phantom{-}0 \\
1 & \phantom{-}1 & \phantom{-}1 & \phantom{-}1 \\
1 & \phantom{-}1 & \phantom{-}1 & -1 \\
\end{bmatrix} \otimes J_{(N/4) \times 1} \qquad \qquad
J_{(N/4) \times 1} \otimes
\begin{bmatrix}
\phantom{-}1  \\
\phantom{-}1  \\
-1 \\
-1  \\
\end{bmatrix} \right )
\end{equation*}
where $\otimes$ denotes the Kronecker product and $J_{m \times n}$ is the $m$ by $n$ matrix with all entries equal to 1.

\vspace{0.1in} \pause

Interpretations of the gene-specific parameters (dropping the $g$ subscript) are
\begin{itemize}[<+->]
\item $\beta_1$ is the parental mean
\item $\beta_2$ is the half difference of hybrid mean vs M
\item $\beta_3$ is the half difference of hybrid mean vs B
\item $\beta_4$ is the half difference between hybrids
\item $\beta_5$ is the flow cell block effect 
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Heterosis hypotheses}

{\footnotesize
\begin{tabular}{l|l|l}
Heterosis & With log-scale group means  & With $\beta_{g\ell}$ parameters \\ \hline
high-parent BM & $\phantom{1\mu_{g,\text{BM}} + } \mu_{g,\text{BM}} > \phantom{2} \max \left (\mu_{g,\text{B}}, \mu_{g,\text{M}} \right )$ & $\phantom{-}2\beta_{g2} + \beta_{g4}, \phantom{-}2\beta_{g3} + \beta_{g4} > 0$ \pause \\
low-parent BM & $\phantom{1\mu_{g,\text{BM}} + }\mu_{g,\text{BM}} < \phantom{2}  \min\, \left (\mu_{g,\text{B}}, \mu_{g,\text{M}} \right )$ & $-2\beta_{g2} - \beta_{g4}, -2\beta_{g3} - \beta_{g4} > 0$ \\
high-parent MB & $\phantom{1\mu_{g,\text{BM}} + }\mu_{g,\text{MB}} > \phantom{2}  \max \left (\mu_{g,\text{B}}, \mu_{g,\text{M}} \right )$ & $\phantom{-}2\beta_{g2} - \beta_{g4}, \phantom{-}2\beta_{g3} - \beta_{g4} > 0$ \\
low-parent MB & $\phantom{1\mu_{g,\text{BM}} + }  \mu_{g,\text{MB}} < \phantom{2} \min\, \left (\mu_{g,\text{B}}, \mu_{g,\text{M}} \right )$ & $-2\beta_{g2} + \beta_{g4}, \phantom{-}2\beta_{g3} + \beta_{g4} > 0$ \\
high-parent mean & $\mu_{g,\text{BM}} + \mu_{g,\text{MB}} > 2 \max \left (\mu_{g,\text{B}}, \mu_{g,\text{M}} \right )$ & $\phantom{-}\beta_{g2}, \phantom{-}\beta_{g3} > 0$ \\
low-parent mean & $\mu_{g,\text{BM}} + \mu_{g,\text{MB}} < 2 \min\, \left (\mu_{g,\text{B}}, \mu_{g,\text{M}} \right )$ & $-\beta_{g2}, -\beta_{g3} > 0$ 
\end{tabular}
}

\vspace{0.2in} \pause

All hypothesis regions are intersections of linear combination events\pause, but we can also accomodate unions of contrast events via 
\[ 
P(A\cup B) = P(A) + P(B) - P(A \cap B).
\]

\end{frame}

\begin{frame}
\frametitle{Directed acyclic graphical model}
\setkeys{Gin}{width=\textwidth}

\vspace{-0.1in}

\begin{center}
\begin{minipage}{0.49\textwidth}
\includegraphics{dag3c}
\end{minipage}
\pause
\begin{minipage}{0.49\textwidth}
\[ \begin{array}{l}
p(\varepsilon,\beta,\gamma,\theta,\sigma,\tau,\nu|y) \pause = \\ \\
p(\varepsilon,\beta,\gamma|\tau,\nu,\theta,\sigma, y) \\
\, \times p(\tau,\nu,\theta,\sigma|y) \pause \propto \\ \\
\quad \prod_{g=1}^G \left\{ \left[ \prod_{n=1}^N p(y_{gn}|\beta_{g},\varepsilon_{gn}) p(\varepsilon_{gn}|\gamma_g) \right] \right. \\
\quad \phantom{\prod_{g=1}^G} \left[\prod_{\ell=1}^L p(\beta_{g\ell}|\theta_\ell,\sigma_\ell) p(\theta_\ell) p(\sigma_\ell) \right] \\
\quad \left.\phantom{\prod_{g=1}^G} p(\gamma_g|\tau,\nu) \right\} p(\tau)p(\nu)
\end{array} \]
\end{minipage}
{\tiny (Will Landau)}
\end{center}

% \begin{figure}[htbp]
%    \centering
%    \begin{minipage}{0.49\textwidth}
%    \begin{align*}
% &y_{gn} \stackrel{\text{ind}}{\sim} \text{Poisson} \left (\exp \left (h_n + \varepsilon_{gn} + X_n \beta_{g} \right ) \right ) \\
% & \qquad \varepsilon_{gn} \stackrel{\text{ind}}{\sim} \text{Normal}(0, \gamma_g) \\
% & \qquad \qquad \gamma_g \stackrel{\text{ind}}{\sim} \text{Inverse-Gamma}\left (\frac{\nu}{2}, \frac{\nu \tau}{2} \right) \\
% & \qquad \qquad \qquad \nu \stackrel{\text{}}{\sim} \text{Uniform}(0, d) \\
% & \qquad \qquad \qquad \tau \stackrel{\text{}}{\sim} \text{Gamma}(a, \text{rate} = b) \\
% & \qquad \beta_{g\ell} \stackrel{\text{ind}}{\sim} \text{Normal}(\theta_\ell, \sigma_\ell^2) \\
% & \qquad \qquad \theta_\ell \stackrel{\text{ind}}{\sim} \text{Normal}(0, c_\ell^2) \\
% & \qquad \qquad \sigma_\ell \stackrel{\text{ind}}{\sim} \text{Uniform}(0, s_\ell)
% \end{align*}
%    \end{minipage} 
%    \begin{minipage}{0.49\textwidth}
%    \includegraphics{dag3c}
%    \end{minipage}
% \end{figure}
\pause
where $G\approx 40\,000$, $N=16$, and $L=5$ in our application \pause and thus we have 
\begin{itemize}
\item $G\times N + G + 2 + G\times L + 2\times L \approx 800\,000$ parameters \pause
\item and $G\times N \approx 640\,000$ observations.
\end{itemize}
\end{frame}




\section{Inference}



\subsection{Fully Bayes}
\begin{frame}
\frametitle{Priors}

All priors are constructed to be vague, proper, and (if possible) conditionally conjugate. \pause 
There are $2(L+1)$ hyperparameters and we assign the following priors
\[ \begin{array}{rll}
\tau &\sim Ga(a,b) & \mbox{conditionally conjugate} \\
\nu &\sim Unif(0,d) \\
\theta_\ell &\ind N(0,c_\ell^2) & \mbox{conditionally conjugate} \\
\sigma_\ell &\ind Unif(0,s_\ell) & \mbox{{\tiny (Gelman (2006))}}
\end{array} \]
\pause
As we'll see, posterior distributions for these parameters are extremely tight relative to their priors. 

\end{frame}





\begin{frame}
\frametitle{Constructing a Gibbs sampler}

Conditional independence within a step:
\[ \begin{array}{rl}
p(\varepsilon|\ldots) &\propto \prod_{g=1}^G \prod_{n=1}^N Po\left(y_{gn}\left|e^{h_n+\varepsilon_{gn}+x_n'\beta_g}\right.\right)N(\varepsilon_{gn}|0,\gamma_g) \\
p(\gamma|\ldots) &\propto \prod_{g=1}^G \prod_{n=1}^N N\left(\varepsilon_{gn}\left|0,\gamma_g\right.\right) IG(\gamma_g|\nu/2,\nu\tau/2) \\
p(\beta_{\ell}|\ldots) &\propto \prod_{g=1}^G \prod_{n=1}^N Po\left(y_{gn}\left|e^{h_n+\varepsilon_{gn}+x_n'\beta_g}\right.\right)N(\beta_{g\ell}|\theta_\ell, \sigma_\ell^2) 
\end{array} \]

\vspace{0.1in} \pause

Sufficient ``statistics'':
\[ \begin{array}{rl@{\qquad}rl}
p(\tau|\ldots) &\sim Ga(\tau|a',b') & (a',b') &= f_{\tau}(\gamma,\nu,a,b) \\
p(\nu|\ldots) &\sim p(\nu|d')\mathrm{I}(0<\nu<d) & d' &= f_\nu(\gamma,\tau,d) \\
p(\theta_\ell|\ldots) &\sim N(\theta_\ell|m'_\ell, C'_\ell) & (m'_\ell,C'_\ell) &= f_{\theta_\ell}(\beta_\ell,\sigma_\ell,c_\ell^2) \\
p(\sigma_\ell^2|\ldots) &\sim IG(e',f') \mathrm{I}(0<\sigma_\ell^2<s^2_\ell) & (e',f') &= f_{\sigma_\ell}(\beta_\ell,\theta_{\ell})
\end{array} \]
\pause
where the functions calculate means, variances, products, etc. over $G$ terms. 
\end{frame}


\subsection{GPUs}
\begin{frame}
\frametitle{Parallelization translated to a GPU}

If there are $G$ nodes, \pause then 
\begin{itemize}
\item Conditional independence \pause $\to$ embarrassingly parallel \pause - possible speedup is $G$ \pause
\item Calculate sufficient ``statistics'' \pause $\to$ reduction \pause - possible speedup is $[G-1]/\log_2(G)$
\end{itemize}

\vspace{0.1in} \pause

\begin{center}
\includegraphics{parallel_reduction}

{\tiny (\url{https://scs.senecac.on.ca/~gpu610/pages/images/parallel_reduction.png})}
\end{center}


\end{frame}



\begin{frame}
\frametitle{GPU computing algorithm constraints}

\begin{center}
\begin{tabular}{ll}
Constraint & Solution \\
\hline
memory coalescence \pause & set up proper data structures \pause \\ \\
only uniform and normal RNGs \pause & step out slice sampler \pause \\ \\
data transfer speed \pause & thinning \pause \\ \\
& return draws for a small subset of parameters \\
& \quad hyperparameters \\
& \quad random set of gene-specific parameters \pause \\ \\
& calculate running sums for \\
& \quad convergence diagnostics \\
& \quad normal-based credible intervals \\
& \quad hypothesis probabilities \\
\hline
\end{tabular}
\end{center}
\end{frame}



\begin{frame}
\frametitle{Convergence diagnostics}
\small

For each parameter $\theta$ over the $M$ iterations of chain $c$, calculate 
\[ 
M\ov{\theta}_c= \sum_{m = 1}^M \theta_c^{(m)} 
\qquad\mbox{and}\qquad
M\ov{\theta^2}_c = \sum_{m = 1}^M \left(\theta_c^{(m)} \right)^2.
\]
using a numerically stable one-pass (or online) algorithm.

\vspace{0.1in} \pause \pause

Compute  the Gelman-Rubin convergence diagnostic amongst $C$ chains using
\[
\widehat{R} = \sqrt{1 + \frac{1}{M} \left(\frac{B}{W} - 1 \right)}
\] 
where 
\[
B = \frac{M}{C - 1} \sum_{c = 1}^C  \left (\ov{\theta}_c - \ov{\theta} \right )^2,
W = \frac{1}{C} \sum_{c = 1}^C S_c^2,
\]
\[
\ov{\theta} = \frac{1}{C} \sum_{c = 1}^C \ov{\theta}_c,
\mbox{ and }
S_c^2 = \frac{M}{M-1}\left[\ov{\theta^2}_c-\ov{\theta}^2_c \right] \approx \ov{\theta^2}_c-\ov{\theta}^2_c 
%\frac{1}{M - 1} \sum_{m = 1}^M \left (\theta_c^{(m)} - \ov{\theta}_c \right )^2
.
\]

\end{frame}


\begin{frame}
\frametitle{Normal-based credible intervals}

For the collection of parameters $\psi$ and under regularity conditions, we have 
\[ 
p_N(\psi|y_N) \stackrel{d}{\to} N\left(\psi_0, \left[\mathrm{I}_N(\psi_0)\right]^{-1}\right)
\]
as $N\to \infty$ where $\psi_0$ is the true value and $\mathrm{I}_N(\psi_0)$ is the Fisher information. 

\vspace{0.1in} \pause

For any scalar parameter $\theta$, we have 
\[ 
\theta|y \dot\sim N\left(\ov{\theta}, \ov{\theta^2}-\ov{\theta}^2\right)
\]
\pause
and can construct normal-based credible intervals with 
\[ 
\ov{\theta} \pm z_{\alpha/2} \sqrt{\ov{\theta^2}-\ov{\theta}^2}
\]
where $P(Z>z_\alpha)=\alpha$ and $Z$ is a standard normal distribution.
\end{frame}




\begin{frame}
\frametitle{Estimating hypothesis probabilities}

Recall we are interested in estimating probabilities similar to 

\[ \begin{array}{l}
P(\mbox{high parent heterosis for the B73$\times$Mo17 hybrid}|y) \\ \\
\qquad = P\left(\left.2\beta_{g2} + \beta_{g4}>0 \mbox{ and }  2\beta_{g3} + \beta_{g4} > 0\right|y \right) \pause \\ \\
\qquad \approx \frac{1}{M} \sum_{m=1}^M \mathrm{I}\left( 2\beta_{g2}^{(m)} + \beta_{g4}^{(m)}>0\right)\mathrm{I}\left(2\beta_{g3}^{(m)} + \beta_{g4}^{(m)} > 0 \right) 
\end{array} \]

\vspace{0.1in} \pause

We can use the running sums to keep track of this sum. 

\end{frame}



\begin{frame}[fragile]
\frametitle{Implementation}
The computation for this hierarchical overdispersed count regression model is provided in the following three R packages at \url{https://github.com/wlandau/}:

\begin{itemize}
\item {\tt fbseq}: user interface 
\item {\tt fbseqOpenMP}: multithreaded backend
\item {\tt fbseqCUDA}: NVIDIA GPU backend
\end{itemize}

<<setup,size="tiny">>=
library(fbseq)
data(paschold) # Paschold et. al. (2012) data

paschold@contrasts[[5]]
paschold@contrasts[[6]]
paschold@propositions$`high-parent_B73xMo17`
@

<<dependson="setup",eval=FALSE, size="tiny">>=
configs    = Configs(burnin = 10, iterations = 10, thin = 1) 
chain      = Chain(paschold, configs) 
chain_list = fbseq(chain, backend = "CUDA")
@
\end{frame}




\section{Simulation studies}

\begin{frame}
\frametitle{Simulation studies}

\begin{itemize}
\item Simulation model
  \begin{itemize}
  \item Model: hierarchical count regression model with data-based hyperparameters
  \item Simple: count regression model with sparsity in gene-specific parameters 
  \item edgeR: negative binomial model with data-based gene-specific parameters \pause
  \end{itemize}
\item Inference
  \begin{itemize}
  \item edgeR: non-hierarchical except for overdispersion, negative binomial model
  \item fully Bayes: Bayesian analysis with hierarchical count regression model  \pause
  \item eBayes (Means): empirical Bayesian analysis with hierarchical count regression model with hyperparameter estimated from posterior means of the fully Bayes approach
  \item eBayes (Oracle): empirical Bayesian analysis with hierarchical count regression model with true values for the hyperparameters \pause
  \end{itemize}
\item Data size
  \begin{itemize}
  \item $G=30\,000$
  \item $N=16$ and $N=32$
  \end{itemize}
\end{itemize}

\end{frame}

\subsection{Credible interval coverage}
\begin{frame}
\frametitle{Hyperparameter coverage}
\setkeys{Gin}{width=0.8\textwidth}
\begin{center}
\includegraphics{fig-hypercoverage}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Gene-specific parameter coverage}

Overall, we had approximately 95\% coverage of 95\% credible intervals, \pause but 

\vspace{0.1in}

\begin{center}
\includegraphics{fig-betashrink}
\end{center}

where the lower plot indicates that when the intervals miss, we are overshrinking. 
\end{frame}


\begin{frame}
\frametitle{Mean squared error}
\setkeys{Gin}{width=0.8\textwidth}
\begin{center}
\includegraphics{fig-msecomparison}
\end{center}
\end{frame}



\subsection{Heterosis detection}
\begin{frame}
\frametitle{ROC curves for heterosis detection}
\setkeys{Gin}{width=0.8\textwidth}
\begin{center}
\includegraphics{fig-roc16}
\end{center}
\end{frame}




\section{Real data analysis}
\begin{frame}
\setkeys{Gin}{width=0.4\textwidth}
\frametitle{Analysis of Paschold et. al. (2012) data}

\begin{center}
\includegraphics{heterosis}
\end{center}

\begin{itemize}
\item $N=16$ with 4 replicates/variety on 2 plates
	\begin{itemize}
	\item varieties: B73, Mo17, B73$\times$Mo17, Mo17$\times$ B73
	\end{itemize}
\item $G=39\,656$
\item 21\% of genes have mean counts less than 1
\item 39\% have mean counts less than 10
\item count: median 37, mean $260$, and max $38\,010$
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Hyperparameter posterior}
\setkeys{Gin}{width=0.8\textwidth}
\begin{center}
\includegraphics{fig-hyperhist}
\end{center}
\end{frame}


\begin{frame}
\setkeys{Gin}{width=0.6\textwidth}
\frametitle{Gene-specific parameter posteriors}

Compare posterior distribution (gray area) to normal-based approximation (black dashed line).

\vspace{-0.15in}

\begin{center}
\includegraphics{fig-betahist}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Smokestack plot}
\setkeys{Gin}{width=0.7\textwidth}

Effect size for HPH BM is the maximum of 0 and $\min(2\beta_{g2}+\beta_{g4}, 2\beta_{g3}+\beta_{g4})/\sqrt{\gamma_g}$. 

\begin{center}
\includegraphics{fig-volcano}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Shrinkage}
\setkeys{Gin}{width=0.7\textwidth}

\begin{center}
\includegraphics{fig-edgerparms}
\end{center}
\end{frame}





\begin{frame}
\frametitle{Summary}

Material available at
\begin{itemize}
\item R package: \url{http://github.com/wlandau/fbseq}
\item slide code: \url{https://github.com/jarad/ISU2016}
\item slide pdf: \url{http://www.jarad.me/research/presentations.html}
\item pre-print: \url{https://arxiv.org/abs/1606.06659}
\end{itemize}

\vspace{0.2in} \pause

\begin{center}
{\Huge
Thank you!
}
\end{center}

\end{frame}


\appendix
\backupbegin
\begin{frame}
\frametitle{References}
\scriptsize

\begin{itemize}
\item Carvalho, C. M., Polson, N. G., and Scott, J. G. (2010). The horseshoe estimator for sparse signals. \emph{Biometrika} 97(2)" 465--480
\item Gelman, A. (2006) Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). \emph{Bayesian analysis}, 1(3), 515--534.
\item Gelman, A., Hill, J., and Yajima, M. (2012) Why we (usually) don't have to worry about multiple comparisons. \emph{Journal of Research on Educational Effectiveness}, 5(2), 189--211.
\item Hans, C. (2009). Bayesian lasso regression. Biometrika, 96(4), 835-845.
\item Lee, A., Yau, C., Giles, M. B., Doucet, A., and Holmes, C. C. (2010). On the utility of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods. \emph{Journal of computational and graphical statistics}, 19(4), 769-789.
\item Lithio, A., and Nettleton, D. (2015). Hierarchical modeling and differential expression analysis for rna-seq experiments with inbred and hybrid genotypes. \emph{Journal of Agricultural, Biological, and Environmental Statistics}, 20(4), 598-613.
\item Liu, Fangfang, Chong Wang, and Peng Liu. (2015) A Semi-parametric Bayesian Approach for Differential Expression Analysis of RNA-seq Data. \emph{Journal of Agricultural, Biological, and Environmental Statistics} 20, no. 4 : 555--576.
\item Park, T., and Casella, G. (2008). The Bayesian lasso. \emph{Journal of the American Statistical Association}, 103(482), 681-686.
\item Paschold, A., Jia, Y., Marcon, C., Lund, S., Larson, N.B., Yeh, C.T., Ossowski, S., Lanz, C., Nettleton, D., Schnable, P.S. and Hochholdinger, F., (2012) Complementation contributes to transcriptome complexity in maize (Zea mays L.) hybrids relative to their inbred parents. \emph{Genome researc}h, 22(12), pp.2445--2454.
\item Suchard, M. A., Wang, Q., Chan, C., Frelinger, J., Cron, A., and West, M. (2010). Understanding GPU programming for statistical computation: Studies in massively parallel massive mixtures. \emph{Journal of Computational and Graphical Statistics}, 19(2), 419-438.
\end{itemize}
\end{frame}


\backupend
\end{document}   














